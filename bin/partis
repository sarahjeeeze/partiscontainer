#!/usr/bin/env python
from distutils.version import StrictVersion
import argparse
import copy
import time
import random
import sys
import subprocess
# import multiprocessing
import numpy
import scipy
import collections
if StrictVersion(scipy.__version__) < StrictVersion('0.17.0'):
    raise RuntimeError("scipy version 0.17 or later is required (found version %s)." % scipy.__version__)
import colored_traceback.always
import os
partis_dir = os.path.dirname(os.path.realpath(__file__)).replace('/bin', '')
if not os.path.exists(partis_dir):
    print 'WARNING current script dir %s doesn\'t exist, so python path may not be correctly set' % partis_dir
sys.path.insert(1, partis_dir + '/python')

import utils
import glutils
import treeutils
import processargs
import seqfileopener
from partitiondriver import PartitionDriver

# ----------------------------------------------------------------------------------------
def run_simulation(args):
    if args.paired_loci is not None:
        run_paired_heavy_light(args)
        return

    from recombinator import Recombinator

    utils.prep_dir(args.workdir)

    if args.generate_trees:  # note that if we're not simulating from scratch we kind of don't need to generate trees in a separate step (we can just set --choose-trees-in-order and heavy and light will use the same trees in the same order), but for scratch simulation they end up different (I think just from generating the germline set below). It anyway seems safer to really specify the same trees.
        import treegenerator
        tmp_shm_parameter_dir = (args.shm_parameter_dir + '/' + args.parameter_type) if args.shm_parameter_dir is not None else None
        treegen = treegenerator.TreeGenerator(args, tmp_shm_parameter_dir)
        treegen.generate_trees(args.seed, args.outfname, args.workdir)
        return

    default_prevalence_fname = args.workdir + '/allele-prevalence.csv'
    if args.generate_germline_set and args.allele_prevalence_fname is None:
        args.allele_prevalence_fname = default_prevalence_fname

    if args.initial_germline_dir is not None:  # the command line explicitly told us where to get the glfo
        if args.reco_parameter_dir is not None:
            print '  note: getting germline sets from --initial-germline-dir, even though --reco-parameter-dir was also set'
        input_gldir = args.initial_germline_dir
    else:  # if it wasn't explicitly set, we have to decide what to use
        if args.rearrange_from_scratch:  # just use the default
            input_gldir = args.default_initial_germline_dir
        else:  # otherwise assume we're supposed to use the glfo in --reco-parameter-dir
            input_gldir = args.reco_parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir
    glfo = glutils.read_glfo(input_gldir, args.locus, only_genes=args.only_genes)

    working_gldir = None  # if we generate a germline set and we're also going to run subprocesses, we need to write that glfo to disk so the subprocs can see it
    if not args.im_a_subproc and args.generate_germline_set:
        glutils.generate_germline_set(glfo, args.n_genes_per_region, args.n_sim_alleles_per_gene, args.min_sim_allele_prevalence_freq, args.allele_prevalence_fname, new_allele_info=args.new_allele_info, debug=args.debug>1)  # NOTE removes unwanted genes from <glfo>
        if utils.getsuffix(args.outfname) == '.csv':
            print '  writing generated germline set to %s/' % args.outfname.replace('.csv', '-glfo')
            glutils.write_glfo(args.outfname.replace('.csv', '-glfo'), glfo)
        if args.n_procs > 1:
            working_gldir = args.workdir + '/' + glutils.glfo_dir
            glutils.write_glfo(working_gldir, glfo)

    # ----------------------------------------------------------------------------------------
    def make_events(n_events, workdir, outfname, random_ints):
        reco = Recombinator(args, glfo, seed=args.seed, workdir=workdir)
        start = time.time()
        events = []
        for ievt in range(n_events):
            event = reco.combine(random_ints[ievt], i_choose_tree=ievt if args.choose_trees_in_order else None)
            events.append(event)
        if args.check_tree_depths or args.debug:
            reco.print_validation_values()
        utils.write_annotations(outfname, glfo, events, utils.add_lists(list(utils.simulation_headers), args.extra_annotation_columns), synth_single_seqs=utils.getsuffix(outfname) == '.csv', use_pyyaml=args.write_full_yaml_output, dont_write_git_info=args.dont_write_git_info)  # keep writing the csv as single-sequence lines, just for backwards compatibility (now trying to switch to synthesizing single seq lines when reading) NOTE list() cast is terrible, but somehow I've ended up with some of the headers as lists and some as tuples, and I can't track down all the stuff necessary to synchronize them a.t.m.
        print '    made %d event%s with %d seqs in %.1fs (%.1fs of which was running bppseqgen)' % (len(events), utils.plural(len(events)), sum(len(l['unique_ids']) for l in events), time.time()-start, sum(reco.validation_values['bpp-times']))

    if not args.im_a_subproc:
        print 'simulating'

    n_per_proc_list, work_fnames = [], []
    if args.n_procs == 1:
        make_events(args.n_sim_events, args.workdir, args.outfname, [random.randint(0, numpy.iinfo(numpy.int32).max) for _ in range(args.n_sim_events)])
    else:
        # ----------------------------------------------------------------------------------------
        def get_workdir(iproc):
            return args.workdir + '/sub-' + str(iproc)
        # ----------------------------------------------------------------------------------------
        def get_outfname(iproc):
            return get_workdir(iproc) + '/' + os.path.basename(args.outfname)
        # ----------------------------------------------------------------------------------------
        if args.input_simulation_treefname is not None:  # split up the input trees among the sub procs
            with open(args.input_simulation_treefname) as tfile:
                treelines = tfile.readlines()
            if len(treelines) < args.n_sim_events:
                print '  note: total number of trees %d less than --n-sim-events %d' % (len(treelines), args.n_sim_events)
        cmdfos = []
        for iproc in range(args.n_procs):
            n_sub_events = args.n_sim_events / args.n_procs
            if iproc == 0 and args.n_sim_events % args.n_procs > 0:  # do any extra ones in the first proc
                n_sub_events += args.n_sim_events % args.n_procs
            n_per_proc_list.append(n_sub_events)

            clist = copy.deepcopy(sys.argv)
            utils.remove_from_arglist(clist, '--n-procs', has_arg=True)
            clist.append('--im-a-subproc')
            utils.replace_in_arglist(clist, '--seed', str(args.seed + iproc))
            utils.replace_in_arglist(clist, '--workdir', get_workdir(iproc))
            utils.replace_in_arglist(clist, '--outfname', get_outfname(iproc))
            utils.replace_in_arglist(clist, '--n-sim-events', str(n_sub_events))
            if working_gldir is not None:
                utils.replace_in_arglist(clist, '--initial-germline-dir', working_gldir)
            if args.allele_prevalence_fname is not None:
                utils.replace_in_arglist(clist, '--allele-prevalence-fname', args.allele_prevalence_fname)

            if args.input_simulation_treefname is not None:
                subtreefname = '%s/trees-sub-%d.nwk' % (args.workdir, iproc)
                utils.replace_in_arglist(clist, '--input-simulation-treefname', subtreefname)
                n_sub_trees = len(treelines) / args.n_procs
                sub_tree_lines = treelines[iproc * n_sub_trees : (iproc + 1) * n_sub_trees]
                if iproc == 0 and len(treelines) % args.n_procs > 0:  # add any extra ones to the first proc
                    sub_tree_lines += treelines[args.n_procs * n_sub_trees : ]
                if len(sub_tree_lines) == 0:
                    raise Exception('couldn\'t split up %d trees among %d procs' % (len(treelines), args.n_procs))
                if len(sub_tree_lines) < n_sub_events:
                    print '  note: number of trees %d smaller than number of events %d for sub proc %d (of %d)' % (len(sub_tree_lines), n_sub_events, iproc, args.n_procs)
                work_fnames.append(subtreefname)
                with open(subtreefname, 'w') as stfile:
                    stfile.writelines(sub_tree_lines)

            cmdstr = ' '.join(clist)
            if args.debug:
                print '  %s %s' % (utils.color('red', 'run'), cmdstr)
            cmdfos.append({'cmd_str' : cmdstr, 'workdir' : get_workdir(iproc), 'logdir' : args.workdir + '/log-' + str(iproc),'outfname' : get_outfname(iproc)})  # logdirs have to be different than <workdirs> since ./bin/partis (rightfully) barfs if its workdir already exists

        utils.run_cmds(cmdfos, batch_system=args.batch_system, batch_options=args.batch_options, batch_config_fname=args.batch_config_fname, debug='print')
        file_list = [cmdfos[i]['outfname'] for i in range(args.n_procs)]
        utils.merge_simulation_files(args.outfname, file_list, utils.add_lists(list(utils.simulation_headers), args.extra_annotation_columns),  # list() cast is terrible, but somehow I've ended up with some of the headers as lists and some as tuples, and I can't track down all the stuff necessary to synchronize them a.t.m.
                                      n_total_expected=args.n_sim_events, n_per_proc_expected=n_per_proc_list, use_pyyaml=args.write_full_yaml_output, dont_write_git_info=args.dont_write_git_info)

    if not args.im_a_subproc and args.allele_prevalence_fname is not None:  # check final prevalence freqs
        glutils.check_allele_prevalence_freqs(args.outfname, glfo, args.allele_prevalence_fname, only_region='v', debug=args.debug>1)
        if args.allele_prevalence_fname == default_prevalence_fname:
            os.remove(default_prevalence_fname)

    if working_gldir is not None:
        glutils.remove_glfo_files(working_gldir, args.locus)
    if args.n_procs > 1:
        for iproc in range(args.n_procs):
            os.rmdir(cmdfos[iproc]['logdir'])
    if not args.im_a_subproc:  # remove the dummy output file if necessary (if --outfname isn't set on the command line, we still write to a temporary output file so we can do some checks)
        if args.outfname == processargs.get_dummy_outfname(args.workdir):  # note that we *don't* want to use the light_chain arg here, that's only used in the parent process that calls the heavy and light subprocesses. In the parent process we don't want anything removed here since we still need to read both files, and in the subprocesses we want light_chain to be False
            work_fnames.append(args.outfname)
        utils.rmdir(args.workdir, fnames=work_fnames)

# ----------------------------------------------------------------------------------------
def read_inputs(args, actions):
    if actions[0] == 'cache-parameters':  # for parameter caching, use the default in data/germlines/human unless something else was set on the command line
        gldir = args.default_initial_germline_dir if args.initial_germline_dir is None else args.initial_germline_dir
    elif runs_on_existing_output(actions[0]):
        if utils.getsuffix(args.outfname) == '.csv':  # old way
            if args.initial_germline_dir is not None:
                gldir = args.initial_germline_dir
            elif args.parameter_dir is not None and os.path.exists(args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir):
                gldir = args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir
            else:
                raise Exception('couldn\'t guess germline info location with deprecated .csv output file: either set it with --intitial-germline-dir or --parameter-dir, or use .yaml output files so germline info is written to the same file as the rest of the output')
        elif utils.getsuffix(args.outfname) == '.yaml':  # new way
            gldir = None  # gets set when we read the glfo from the yaml in partitiondriver
        else:
            raise Exception('unhandled annotation file suffix %s' % args.outfname)
    else:
        if args.initial_germline_dir is not None:
            gldir = args.initial_germline_dir
        elif args.parameter_dir is not None and os.path.exists(args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir):
            gldir = args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir
        else:
            tstr = ''
            if args.parameter_dir is not None and not os.path.exists(args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir):
                tstr = ' (--parameter-dir was set to %s, but the corresponding glfo dir %s doesn\'t exist)' % (args.parameter_dir, args.parameter_dir + '/' + args.parameter_type + '/' + glutils.glfo_dir)
            raise Exception('couldn\'t guess germline info location: set it with either --intitial-germline-dir or --parameter-dir%s' % tstr)

    glfo = None
    if gldir is not None:
        template_glfo = None
        if args.sanitize_input_germlines:
            print '   using default germline dir %s for template glfo (e.g. for codon positions)' % args.default_initial_germline_dir
            template_glfo = glutils.read_glfo(args.default_initial_germline_dir, args.locus)
        glfo = glutils.read_glfo(gldir, args.locus, only_genes=args.only_genes, template_glfo=template_glfo, add_dummy_name_components=args.sanitize_input_germlines, debug=2 if args.sanitize_input_germlines else False)

    simglfo = None
    if not args.is_data:
        if args.infname is not None and utils.getsuffix(args.infname) == '.yaml':
            simglfo = None  # cause we'll read it from the input file
        elif args.simulation_germline_dir is not None:  # if an explicit dir was set on the command line
            simglfo = glutils.read_glfo(args.simulation_germline_dir, locus=args.locus)  # probably don't apply <args.only_genes> (?)
        else:
            raise Exception('couldn\'t find simulation germline info: either set with --simulation-germline-dir, or use .yaml simulation files and set --infname')

    more_input_info = None
    if args.queries_to_include_fname is not None:
        more_input_info, _, _ = seqfileopener.read_sequence_file(args.queries_to_include_fname, args.is_data)
        print '  --queries-to-include-fname: adding %d extra sequence%s from %s' % (len(more_input_info), utils.plural(len(more_input_info)), args.queries_to_include_fname)
        if args.queries_to_include is None:
            args.queries_to_include = []
        args.queries_to_include += more_input_info.keys()

    if args.infname is None:  # put this *after* setting queries_to_include from a file, since we need that set
        return None, None, glfo, simglfo

    input_info, reco_info, yaml_glfo = seqfileopener.read_sequence_file(args.infname, args.is_data, n_max_queries=args.n_max_queries, args=args, simglfo=simglfo, more_input_info=more_input_info)
    if not args.is_data and yaml_glfo is not None:  # NOTE is is extremely important that <glfo> doesn't get set to the true info in a simulation yaml
        simglfo = yaml_glfo

    if len(input_info) > 1000 and args.n_procs == 1:
        print '  note: running on %d sequences with only %d process%s. This may be kinda slow, so it might be a good idea to set --n-procs N to the number of processors on your local machine, or look into non-local parallelization with --batch-system.' % (len(input_info), args.n_procs, utils.plural(args.n_procs, prefix='e'))
    if len(input_info) > 1000 and args.outfname is None:
        print '  note: running on a lot of sequences (%d) without setting --outfname. Which is ok, but there will be no persistent record of the results (except the parameter directory).' % len(input_info)
    return input_info, reco_info, glfo, simglfo

# ----------------------------------------------------------------------------------------
def run_partitiondriver(args):
    if args.split_loci:
        run_all_loci(args)
        return
    if args.paired_loci is not None:
        run_paired_heavy_light(args)
        return

    if args.parameter_dir is None:
        args.parameter_dir = '_output/' + args.infname[ : args.infname.rfind('.')].replace('/', '_') if args.infname is not None else 'xxx-dummy-xxx'  # the latter is a shitty convention, but I code further on crashes if I let the parameter dir be None

    actions = [args.action]  # do *not* use <args.action> after this (well, for anything other than checking what was actually set on the command line)
    if args.action in ['annotate', 'partition'] and not os.path.exists(args.parameter_dir):
        if args.refuse_to_cache_parameters:
            raise Exception('--parameter-dir %s doesn\'t exist, and --refuse-to-cache-parameters was set' % args.parameter_dir)
        actions = ['cache-parameters'] + actions
        print '  parameter dir does not exist, so caching a new set of parameters before running action \'%s\': %s' % (actions[1], args.parameter_dir)
        if args.seed_unique_id is not None or args.seed_seq is not None:  # if we're auto parameter caching for/before seed partitioning, we *don't* (yet) want to remove non-clonal sequences, since we need all the non-clonal sequences to get better parameters (maybe at some point we want to be able to count parameters just on this lineage, but for now let's keep it simple)
            raise Exception('if setting --seed-unique-id or --seed-seq for \'partition\', you must first explicitly run \'cache-parameters\' in order to ensure that parameters are cached on all sequences, not just clonally related sequences.')

    input_info, reco_info, glfo, simglfo = read_inputs(args, actions)
    parter = PartitionDriver(args, glfo, input_info, simglfo, reco_info)
    parter.run(actions)
    if not runs_on_existing_output(args.action):  # mostly wanted to avoid rewriting the persistent hmm cache file
        parter.clean()

# ----------------------------------------------------------------------------------------
# it would be really nice if this didn't need to exist, it's complicated (but it's necessitated by needing to auto-cache parameters on the full un-split igh seqs when splitting loci)
# note that i could also just stop having action-specific arguments, but oh well this seems better for now at least
def remove_action_specific_args(clist, action, debug=True):
    if action not in subargs:  # nothing to do
        return
    act_args = set(afo['name'] for afo in subargs[action])
    iarg = 0
    removed_strs = []
    while iarg < len(clist):
        if clist[iarg][:2] != '--':  # skip arg values
            iarg += 1
            continue
        if any(astr.find(clist[iarg]) == 0 for astr in act_args):  # have to allow for incompletely-written args, since argparser handles those
            removed_strs.append(clist[iarg])
            clist.pop(iarg)
            if iarg < len(clist) and clist[iarg][:2] != '--':  # if it has an arg value, also remove that
                removed_strs.append(clist[iarg])
                clist.pop(iarg)
        else:
            iarg += 1
    if debug and len(removed_strs) > 0:
        print '    removed %d arg strs that were specific to action \'%s\': %s' % (len(removed_strs), action, ' '.join(removed_strs))

# ----------------------------------------------------------------------------------------
def run_all_loci(args, ig_or_tr='ig'):  # TODO generalize to allow 'tr'
    # ----------------------------------------------------------------------------------------
    def init_clist():
        clist = copy.deepcopy(sys.argv)
        utils.remove_from_arglist(clist, '--split-loci')
        utils.remove_from_arglist(clist, '--reverse-negative-strands')
        utils.remove_from_arglist(clist, '--paired-loci-output-dir', has_arg=True)
        return clist
    # ----------------------------------------------------------------------------------------
    def getpdir(ltmp):
        return '%s/parameters/%s' % (utils.non_none([args.parameter_dir, args.paired_loci_output_dir]), ltmp)
    # ----------------------------------------------------------------------------------------
    def auto_cache_params():  # note that this somewhat duplicates code in run_partitiondriver() above (but they need to be separate because of the all-igh-seqs vs only-paired-with-igk/l thing)
        if runs_on_existing_output(args.action):  # don't need a parameter dir to merge existing paired partitions
            return False
        existing_pdir_loci = [l for l in utils.sub_loci(ig_or_tr) if os.path.exists(getpdir(l))]
        if len(existing_pdir_loci) == 0:
            return True  # all missing, so auto cache parameters
        elif len(existing_pdir_loci) == len(utils.sub_loci(ig_or_tr)):
            return False  # all there, don't auto-cache
        else:
            print '  %s parameters exist for some but not all loci, so not auto-caching parameters (missing: %s, present: %s) in %s' % (utils.color('yellow', 'warning'), ' '.join(set(utils.sub_loci(ig_or_tr)) - set(existing_pdir_loci)), ' '.join(existing_pdir_loci), getpdir(''))
            return False
    # ----------------------------------------------------------------------------------------
    utils.prep_dir(args.workdir)  # NOTE it's kind of weird to have workdir and paired_loci_output_dir, but workdir is for stuff we throw out by default, whereas paired_loci_output_dir is for stuff we save
    tmpfns = ['%s/%s.fa'%(args.paired_loci_output_dir, l) for l in utils.sub_loci(ig_or_tr)]
    if [os.path.exists(f) for f in tmpfns].count(True) == len(tmpfns):  # this isn't all its outputs, but it's a bunch of them (also note that if the input file has e.g. igh + igk but no igl, this'll rerun since it sees one is missing)
        print '  split input .fa files already exist, so not re-running split-loci.py'
    else:
        utils.simplerun('./bin/split-loci.py %s --split-heavy-seqs%s --outdir %s' % (args.infname, ' --reverse-negative-strands' if args.reverse_negative_strands else '', args.paired_loci_output_dir), dryrun=args.dry_run)

    if args.action == 'cache-parameters' or auto_cache_params():  # ick. we can't just let the normal auto-parameter caching happen within each locus pair below, since it'll infer separately on the igh seqs paired with igk vs igl
        for ltmp in utils.sub_loci(ig_or_tr):
            if not os.path.exists('%s/%s.fa'%(args.paired_loci_output_dir, ltmp)):
                print '%s: no input file, skipping' % utils.color('blue', ltmp)
                continue
            clist = init_clist()
            if args.action != 'cache-parameters':
                print '  parameter dir does not exist, so caching a new set of parameters before running action \'%s\':  %s' % (args.action, getpdir(ltmp))
                clist[clist.index(args.action)] = 'cache-parameters'
                utils.remove_from_arglist(clist, '--outfname', has_arg=True)
                remove_action_specific_args(clist, args.action)
            utils.insert_in_arglist(clist, ['--locus', ltmp], 'cache-parameters')
            utils.replace_in_arglist(clist, '--infname', '%s/%s.fa'%(args.paired_loci_output_dir, ltmp))  # for caching parameters, we want the .fa in the parent dir with *all* igh sequences (but for other actions we want the one in the igh+igX/ subdir)
            utils.replace_in_arglist(clist, '--parameter-dir', getpdir(ltmp))
            utils.simplerun(' '.join(clist), dryrun=args.dry_run)

    if args.action in ['partition', 'merge-paired-partitions']:
        for h_locus, l_locus in utils.locus_pairs[ig_or_tr]:
            if any(not os.path.exists('%s/%s.fa'%(args.paired_loci_output_dir, l)) for l in [h_locus, l_locus]):
                print '%s+%s: missing input file(s) in %s, skipping' % (utils.color('blue', h_locus), utils.color('blue', l_locus), args.paired_loci_output_dir)
                continue
            clist = init_clist()
            utils.insert_in_arglist(clist, ['--paired-loci', '%s:%s' % (h_locus, l_locus)], args.action)
            lpair_workdir = '%s/%s+%s' % (args.paired_loci_output_dir, h_locus, l_locus)
            for lstr, ltmp in [('', h_locus), ('light-chain-', l_locus)]:
                if args.action == 'partition':
                    utils.replace_in_arglist(clist, '--%sinfname'%lstr, '%s/%s.fa'%(lpair_workdir, ltmp), insert_after='--infname')
                    utils.replace_in_arglist(clist, '--%sparameter-dir'%lstr, getpdir(ltmp), insert_after='--parameter-dir')  # note that this'll rerun sw for igh, since the sw cache file in the parameter dir corresponds to all seqs (i.e. both igk and igl paired ones). Which i think is fine, the whole reaosn it's set up that way is that sw annotations depend on the other seqs in the repertoire
                elif args.action == 'merge-paired-partitions':
                    utils.remove_from_arglist(clist, '--infname', has_arg=True)
                utils.replace_in_arglist(clist, '--%soutfname'%lstr, '%s/%s-%s.yaml'%(lpair_workdir, args.action if args.action!='merge-paired-partitions' else 'partition', ltmp))  # also adds it if it isn't there: we definitely need the output so we can proceed with later steps, even if --outfname was set on the command line
            utils.simplerun(' '.join(clist), dryrun=args.dry_run)
    else:
        assert args.action in ['cache-parameters']

    if not args.dry_run:
        utils.rmdir(args.workdir)

# ----------------------------------------------------------------------------------------
def run_paired_heavy_light(args):
    # ----------------------------------------------------------------------------------------
    def run_step(tmpaction, locus=None):
        clist = copy.deepcopy(sys.argv)
        utils.remove_from_arglist(clist, '--paired-loci', has_arg=True)
        utils.remove_from_arglist(clist, '--dry-run')
        treefname = '%s/trees.nwk' % args.workdir
        if tmpaction == 'generate-trees':
            clist += ['--generate-trees']  # NOTE this will use the heavy chain parameter dir and ignore the light chain one, which I think is ok (it's just used for tree scaling, and it would be better to eventually have different scaling for light chain, but it should be fine)
            work_fnames.append(treefname)
            utils.replace_in_arglist(clist, '--outfname', treefname)
            utils.remove_from_arglist(clist, '--light-chain-outfname', has_arg=True)
            utils.remove_from_arglist(clist, '--light-chain-parameter-dir', has_arg=True)  # may as well remove it to make more clear that it's not being used (see comment above)
            utils.remove_from_arglist(clist, '--n-procs', has_arg=True)
        else:
            clist += ['--locus', locus]
            if args.action == 'simulate':
                clist += ['--choose-trees-in-order', '--input-simulation-treefname', treefname]

            lstr = '' if utils.has_d_gene(locus) else 'light-chain-'
            tmp_pdir = None
            if '--%sparameter-dir'%lstr in clist:  # you might think it'd be easier to also add to clist here, but it turns out no
                tmp_pdir = utils.get_val_from_arglist(clist, '--%sparameter-dir' % lstr)  # this has to come from <sys.argv>/<clist> (not <args>) since for simulation it's set to None in processargs (in favor of the reco- and shm- ones)
            ofn = getattr(args, '%soutfname' % lstr.replace('-', '_'))  # whereas this needs to come from <args> rather than <sys.argv>/<clist> in order to pick up any modifications made in processargs
            ifn = getattr(args, '%sinfname' % lstr.replace('-', '_'))  # i don't think it matters for this where it comes from

            for tmpstr in ['infname', 'parameter-dir', 'outfname']:
                utils.remove_from_arglist(clist, '--%s'%tmpstr, has_arg=True)
                utils.remove_from_arglist(clist, '--light-chain-%s'%tmpstr, has_arg=True)

            if ifn is not None:
                clist += ['--infname', ifn]
            if tmp_pdir is not None:
                clist += ['--parameter-dir', tmp_pdir]
            clist += ['--outfname', ofn]  # note that each locus's subprocess needs an outfname even if there's no final outfname from the command line (well, we don't need it for parameter caching, which we're also now doing here, but oh well)

            if not args.dry_run and os.path.exists(ofn):
                print '    removing existing %s locus output file %s' % (locus, ofn)
                os.remove(ofn)
            if ofn == processargs.get_dummy_outfname(args.workdir, light_chain=not utils.has_d_gene(locus)):
                work_fnames.append(ofn)
        utils.simplerun(' '.join(clist), dryrun=args.dry_run)

    # ----------------------------------------------------------------------------------------
    def combine_chains():  # simulation: synchronize uids and check that trees are compatible, partitioning: merge/synchronize partitions
        # ----------------------------------------------------------------------------------------
        def get_ccfs(joint_partition):
            def incorporate_duplicates(tpart, dup_dict):  # take the map from uid to list of its duplicates (dup_dict), and add the duplicates to any clusters in partition tpart that contain that uid
                for tclust in tpart:
                    for uid in tclust:
                        if uid in dup_dict:
                            tclust += dup_dict[uid]
            true_partitions = {}
            cmp_partitions = {}  # (potentially) modified versions of the heav/light partitions
            ccfs = {}
            for chain, ifn in (('h', args.infname), ('l', args.light_chain_infname)):
                _, _, true_cpath = utils.read_output(ifn, dont_add_implicit_info=True, n_max_queries=args.n_max_queries) #skip_annotations=True) # can't skip annotations since older files will need them to make annotations
                true_partitions[chain] = true_cpath.partitions[true_cpath.i_best]
                cmp_partitions[chain] = cpaths[chain].partitions[cpaths[chain].i_best]
                dup_dict = {u : l['duplicates'][i] for l in annotation_lists[chain] for i, u in enumerate(l['unique_ids']) if len(l['duplicates'][i]) > 0}
                if len(dup_dict) > 0:
                    incorporate_duplicates(cmp_partitions[chain], dup_dict)
                ccfs[chain] = {'before' : utils.new_ccfs_that_need_better_names(cmp_partitions[chain], true_partitions[chain])}

                if len(dup_dict) > 0:
                    incorporate_duplicates(joint_partition, dup_dict)  # NOTE this modifies joint_partition, so it's different for the second chain than for the first one
                j_part = utils.get_deduplicated_partitions([joint_partition])[0]  # TODO why do i need this?
                ccfs[chain]['joint'] = utils.new_ccfs_that_need_better_names(j_part, true_partitions[chain])

            print '             purity  completeness'
            for chain in ['h', 'l']:
                print '   %s before  %6.3f %6.3f' % (chain, ccfs[chain]['before'][0], ccfs[chain]['before'][1])
            for chain in ['h', 'l']:
                print '    joint    %6.3f %6.3f   (%s true)' % (ccfs[chain]['joint'][0], ccfs[chain]['joint'][1], chain)
        # ----------------------------------------------------------------------------------------
        hglfo, heavy_annotations, hcpath = utils.read_output(args.outfname, dont_add_implicit_info=not args.debug)
        lglfo, light_annotations, lcpath = utils.read_output(args.light_chain_outfname, dont_add_implicit_info=not args.debug)
        if args.action == 'simulate':
            print '  rewriting output files with synchronized trees'
            assert len(heavy_annotations) == len(light_annotations)
            for hline, lline in zip(heavy_annotations, light_annotations):
                treeutils.merge_heavy_light_trees(hline, lline)
            headers = utils.add_lists(list(utils.simulation_headers), args.extra_annotation_columns)
            utils.write_annotations(args.outfname, hglfo, heavy_annotations, headers, synth_single_seqs=utils.getsuffix(args.outfname) == '.csv', use_pyyaml=args.write_full_yaml_output)
            utils.write_annotations(args.light_chain_outfname, lglfo, light_annotations, headers, synth_single_seqs=utils.getsuffix(args.light_chain_outfname) == '.csv', use_pyyaml=args.write_full_yaml_output)
        else:
            cpaths = {'h' : hcpath, 'l' : lcpath}  # TODO combine this with the above
            annotation_lists = {'h' : heavy_annotations, 'l' : light_annotations}
            print '%s heavy and light chain cluster paths' % utils.color('red', 'synchronizing')
            joint_partition = hcpath.merge_light_chain(lcpath, heavy_annotations, light_annotations, check_partitions=True, debug=False)  # , h_ipart=5
            if not args.is_data and args.infname is not None:  # would be nice to use read_inputs(), but i think it's not worth handling changing args.locus and whatnot
                get_ccfs(joint_partition)
            if args.outfname is not None:
                # TODO write joint partition + newly-subsetted annotations to --outfname and --light-chain-outfname
                pass

    # ----------------------------------------------------------------------------------------
    utils.prep_dir(args.workdir)
    work_fnames = []
    if args.action == 'simulate':
        run_step('generate-trees')
        for locus in args.paired_loci:
            run_step(None, locus=locus)
        if not args.dry_run:
            combine_chains()
    elif args.action == 'partition':
        for locus in args.paired_loci:
            run_step(None, locus=locus)
        if not args.dry_run:
            combine_chains()
    elif args.action == 'merge-paired-partitions':
        combine_chains()
    else:
        assert False

    if not args.dry_run:
        utils.rmdir(args.workdir, fnames=work_fnames)

# ----------------------------------------------------------------------------------------
class MultiplyInheritedFormatter(argparse.RawTextHelpFormatter, argparse.ArgumentDefaultsHelpFormatter):
    pass
formatter_class = MultiplyInheritedFormatter
parser = argparse.ArgumentParser(formatter_class=MultiplyInheritedFormatter)
subparsers = parser.add_subparsers(dest='action')

parent_parser = argparse.ArgumentParser(add_help=False)

parent_parser.add_argument('--locus', default='igh', choices=utils.loci.keys(), help='which immunoglobulin or t-cell receptor locus?')
parent_parser.add_argument('--split-loci', action='store_true', help='This must be set if --infname contains sequences from more than one locus (igh, igk, igl all together). It will first split the loci into separate input files, run the specified action on each of these, and (if specified) merge the resulting partitions.')
parent_parser.add_argument('--reverse-negative-strands', action='store_true', help='If --split-loci is set, align every sequence both forwards and revcomp\'d, then for each sequence keep the sense with better alignment. If *not* running with --split-loci, then first run bin/split-loci.py separately with --reverse-negative-strands.')
parent_parser.add_argument('--paired-loci', help='Instead of the default single chain, run on these two paired loci (colon-separated list of length 2). Have to also set --light-chain-infname or --light-chain-outfname.')
parent_parser.add_argument('--dry-run', action='store_true', help='Just print subprocess commands that would be run without actually running them (only implemented for --paired-loci).')
parent_parser.add_argument('--species', default='human', choices=('human', 'macaque', 'mouse'), help='Which species?')
parent_parser.add_argument('--queries', help='Colon-separated list of query names to which to restrict the analysis')
parent_parser.add_argument('--queries-to-include', help='When reading the input file, look for and include these additional uids when --n-random-queries is set (*not* compatible with --n-max-queries). Contrast with --queries, which includes *only* the indicated uids. Additionally, these queries are treated differently e.g. when partition plotting or removing duplicate sequences.')
parent_parser.add_argument('--queries-to-include-fname', help='In cases where you want certain sequences to be included in --queries-to-include or --seed-unique-id, but these sequences are not in --infname, you can put them in this file. Typically, this is useful when you have a number of seed sequences that are from a separate experiment than the NGS data in --infname.')
parent_parser.add_argument('--reco-ids', help='Colon-separated list of rearrangement-event IDs to which we restrict ourselves')  # or recombination events
parent_parser.add_argument('--n-max-queries', type=int, default=-1, help='Maximum number of query sequences to read from input file, starting from beginning of file')
parent_parser.add_argument('--n-random-queries', type=int, help='choose this many queries at random from entire input file')
parent_parser.add_argument('--istartstop', help='colon-separated start:stop line indices for input sequence file (with python slice conventions, e.g. if set to \'2:4\' will skip the zeroth and first sequences, and then take the following two sequences, and then skip all subsequence sequences). Applied before any other input filters, e.g. --n-max-queries, --queries, --reco-ids, etc.')

parent_parser.add_argument('--debug', type=int, default=0, choices=[0, 1, 2], help='Debug verbosity level.')
parent_parser.add_argument('--sw-debug', type=int, choices=[0, 1, 2], help='Debug level for Smith-Waterman.')
parent_parser.add_argument('--abbreviate', action='store_true', help='Abbreviate/translate sequence ids to improve readability of partition debug output. Uses a, b, c, ..., aa, ab, ...')
parent_parser.add_argument('--print-git-commit', action='store_true', help='print sys.argv, git commit hash, and tag info')
parent_parser.add_argument('--dont-write-git-info', action='store_true', help='Don\'t write git tag/commit info to yaml output files (used to enable diffing of test results)')
parent_parser.add_argument('--only-print-best-partition', action='store_true', help='when printing cluster annotations (either with --debug 1 for \'partition\', or for \'view-output\'), instead of the default of printing the annotation for every cluster for which we calculated one (see --calculate-alternative-naive-seqs and --n-partitions-to-write), only print annotations for clusters in the best partition. UPDATE also restricts to the best partition when calculating selection metrics.')
parent_parser.add_argument('--only-print-seed-clusters', action='store_true', help='same as --only-print-best-partition, but in addition, only print the seed cluster(s). Note that if --only-print-best-partition is *not* set, then there will be more than one seed cluster.')
parent_parser.add_argument('--only-print-queries-to-include-clusters', action='store_true', help='same as --only-print-best-partition, but in addition, only print the cluster(s) corresponding to --queries-to-include/--queries-to-include-fname. Note that if --only-print-best-partition is *not* set, then there will be more than one cluster for each such query.')  # NOTE we could just assume this if queries to include are specified, but that's not the behavior when we're plotting (in which case we include everybody but highlight the queries to include)

parent_parser.add_argument('--n-procs', type=int, default=1, help='Number of processes over which to parallelize. This is usually the maximum that will be initialized at any given time, but for internal reasons, certain steps (e.g. smith waterman and partition naive sequence precaching) sometimes use slightly more.')
parent_parser.add_argument('--n-max-to-calc-per-process', default=250, help='if a bcrham process calc\'d more than this many fwd + vtb values (and this is the first time with this number of procs), don\'t decrease the number of processes in the next step (default %(default)d)')
parent_parser.add_argument('--min-hmm-step-time', default=2., help='if a clustering step takes fewer than this many seconds, always reduce n_procs')
parent_parser.add_argument('--batch-system', choices=['slurm', 'sge'], help='batch system with which to attempt paralellization')
parent_parser.add_argument('--batch-options', help='additional options to apply to --batch-system (e.g. --batch-options="--foo bar")')
parent_parser.add_argument('--batch-config-fname', default='/etc/slurm-llnl/slurm.conf', help='system-wide batch system configuration file name')  # for when you're running the whole thing within one slurm allocation, i.e. with  % salloc --nodes N ./bin/partis [...]

parent_parser.add_argument('--only-smith-waterman', action='store_true', help='Exit after finishing smith-waterman.')
parent_parser.add_argument('--count-parameters', action='store_true', help='force parameter counting when action is not cache-parameters (presumably so that you can plot them, or to get multi-hmm parameters from partitioning)')
parent_parser.add_argument('--dont-write-parameters', action='store_true', help='don\'t write parameters to disk even if you\'ve counted them (mostly for use in conjunction with --only-smith-waterman, so you can avoid cluttering up your file system)')
parent_parser.add_argument('--write-trimmed-and-padded-seqs-to-sw-cachefname', action='store_true', help='after running sw, trim and pad sequences *before* writing to the sw cache file (rather than after). Note that this will in some cases cause waterer.py to not be able to read sw results from this cache file.')
parent_parser.add_argument('--partis-dir', default=partis_dir, help='for internal use only')
parent_parser.add_argument('--ig-sw-binary', default=partis_dir + '/packages/ig-sw/src/ig_align/ig-sw', help='Path to ig-sw executable.')
parent_parser.add_argument('--vsearch-binary',  help='Path to vsearch binary (vsearch binaries for linux and darwin are pre-installed in bin/, but for other systems you need to get your own)')
parent_parser.add_argument('--is-simu', action='store_true', help='Set if running on simulated sequences')
parent_parser.add_argument('--skip-unproductive', action='store_true', help='Skip sequences which Smith-Waterman determines to be unproductive (i.e. if they have stop codons, out of frame cdr3, or mutated cyst/tryp/phen)')
parent_parser.add_argument('--collapse-duplicate-sequences', action='store_true', help='Collapse all sequences that are identical from 5\' end of v to 3\' end of j under a single uid. The duplicate uids then appear in the output key \'duplicates\'. This collapse happens after any framework insertion/constant region trimming (see --remove-framework-insertions).')
parent_parser.add_argument('--also-remove-duplicate-sequences-with-different-lengths', action='store_true', help='If --collapse-duplicate-sequences is set, by default we collapse together only queries that have exactly the same sequence. If this argument is also set, we also collapse sequences that are sub/super strings of each other (we keep/index by the longest one). So, e.g., this would also collapse sequences that code for the same bcr but have different read lengths/coverage.')
parent_parser.add_argument('--dont-remove-framework-insertions', action='store_true', help='By default we trim anything to the 5\' of V and 3\' of J, since partis ignores these regions and it simplifies the output. This argument turns that off. If you want to preserve constant region information, either set this argument (if you want access to the entire constant region sequences, in \'fv_insertion\' and \'jf_insertion\'), or pass in constant region annotations with the \'constant-region\' key in --input-metafname.')
parent_parser.add_argument('--dont-rescale-emissions', action='store_true', help='Don\'t scale each hmm\'s emission probabilities to account for the branch length of each individual sequence.')
parent_parser.add_argument('--no-indels', action='store_true', help='Tell smith-waterman not to look for indels, by drastically increasing the gap-open penalty (you can also set the penalty directly).')
parent_parser.add_argument('--no-indel-gap-open-penalty', type=int, default=1000, help='set --gap-open-penalty to this when --no-indels is set (also used in python/waterer.py')
parent_parser.add_argument('--seed', type=int, default=int(time.time()), help='Random seed used by many different things, but especially when reshuffling sequences between partitioning steps, and during simulation. Set this if you want to get exactly the same result when rerunning.')
parent_parser.add_argument('--min-observations-per-gene', type=int, default=20, help='If a gene is observed fewer times than this, we average over other alleles/primary versions/etc. (e.g. in recombinator and hmmwriter). Also used as a more general "this isn\'t very many observations" value.')
parent_parser.add_argument('--no-per-base-mfreqs', action='store_true', help='When making the HMM model files, instead of the default of accounting for different rates to different bases (i.e. A->T vs A->G), do *not* account for the different rates to different bases. This is only really useful for testing the new simulation option --per-base-mutation.')
parent_parser.add_argument('--region-end-exclusion-length', type=int, default=0, help='when counting/writing parameters, ignore this many bases abutting non-templated insertions for calculating mutation frequencies (note: doesn\'t make a difference (hence set to 0 by default) probably because we\'re setting a strongish prior on these bases when writing hmms anyway')
parent_parser.add_argument('--allow-conserved-codon-deletion', action='store_true', help='When building hmm yaml model files during parameter caching, allow deletions that extend through the conserved codons (cyst in V and tryp/phen in J) (by default such deletions are forbidden; see https://github.com/psathyrella/partis/issues/308). NOTE that this has *no* effect if you\'ve already cached parameters.')
parent_parser.add_argument('--subcluster-annotation-size', type=int, default=3, help='when running the bcrham viterbi algorithm, instead of running the multi-hmm on the entire family, instead split the family into subclusters of (about) this size, then replace each subcluster with its naive sequence and iterate until running on once cluster of (about) this size consisting entirely of inferred naive/ancestor sequences (see https://github.com/psathyrella/partis/issues/308).')

parent_parser.add_argument('--only-genes', help='Colon-separated list of genes to which to restrict the analysis. If any regions (V/D/J) are not represented among these genes, these regions are left unrestricted. If running \'simulate\', you probably also want to set --force-dont-generate-germline-set.')
parent_parser.add_argument('--n-max-per-region', default='3:5:2', help='Number of best smith-waterman matches (per region, in the format v:d:j) to pass on to the hmm')
parent_parser.add_argument('--gap-open-penalty', type=int, default=30, help='Penalty for indel creation in Smith-Waterman step.')
parent_parser.add_argument('--max-vj-mut-freq', type=float, default=0.4, help='skip sequences whose mutation rates in V and J are greater than this (it\'s really not possible to get meaningful smith-waterman matches above this)')
parent_parser.add_argument('--max-logprob-drop', type=float, default=5., help='stop glomerating when the total logprob has dropped by this much')
parent_parser.add_argument('--n-simultaneous-seqs', type=int, help='Number of simultaneous sequences on which to run the multi-HMM (e.g. 2 for a pair hmm)')
parent_parser.add_argument('--all-seqs-simultaneous', action='store_true', help='Run all input sequences simultaneously, i.e. equivalent to setting --n-simultaneous-seqs to the number of input sequences.')
parent_parser.add_argument('--simultaneous-true-clonal-seqs', action='store_true', help='If action is annotate/cache-parameters, run true clonal sequences together simultaneously with the multi-HMM. If actions is partition, skip clustering entirely and instead use the true partition (useful for e.g. validating selection metrics, where you don\'t want to be conflating partition performance with selection metric performance).')
parent_parser.add_argument('--mimic-data-read-length', action='store_true', help='In simulation, trim V 5\' and D 3\' to mimic read lengths seen in data (must also be set when caching parameters)')

parent_parser.add_argument('--infname', help='input sequence file in .fa, .fq, .csv, or partis output .yaml (if .csv, specify id string and sequence headers with --name-column and --seq-column)')
parent_parser.add_argument('--light-chain-infname', help='Separate input file with light chain sequences for when --paired-loci is set (heavy chain sequences should go in --infname)')
parent_parser.add_argument('--name-column', help='column/key name for sequence ids in input csv/yaml file (default: \'unique_ids\'). If set to \'fasta-info-index-N\' for an integer N, it will take the Nth (zero-indexed) value from a fasta files uid line.')
parent_parser.add_argument('--seq-column', help='column/key name for nucleotide sequences in input csv/yaml file (default: \'input_seqs\')')
parent_parser.add_argument('--input-metafname', help='yaml file with meta information for the sequences in --infname (and --queries-to-include-fname), keyed by sequence id. If running multiple steps (e.g. cache-parameters and partition), this must be set for all steps. Currently accepted keys/columns are \'timepoint\', \'affinity\', \'subject\' and \'multiplicity\'. See https://github.com/psathyrella/partis/blob/master/docs/subcommands.md#input-meta-info for an example.')
parent_parser.add_argument('--outfname', help='output file name')
parent_parser.add_argument('--paired-loci-output-dir', help='collective output directory for all output files involved in paired heavy/light sequences, i.e. involving --paired-loci or --split-loci.')
parent_parser.add_argument('--light-chain-outfname', help='output file name for light chain sequences when --paired-loci is set (heavy chain sequences are written to --outfname).')
parent_parser.add_argument('--write-full-yaml-output', action='store_true', help='By default, we write yaml output files using the json subset of yaml, since it\'s much faster. If this is set, we instead write full yaml, which is more human-readable (but also much slower).')
parent_parser.add_argument('--presto-output', action='store_true', help='Write output file(s) in presto/changeo format. Since this format depends on a particular IMGT alignment, this depends on a fasta file with imgt-gapped alignments for all the V, D, and J germline genes. The default in data/germlines/<species>/imgt-alignments/, is probably fine for most cases. For the \'annotate\' action, a single .tsv file is written with annotations (so --outfname suffix must be .tsv). For the \'partition\' action, a fasta file is written with cluster information (so --outfname suffix must be .fa or .fasta), as well as a .tsv in the same directory with the corresponding annotations.')
parent_parser.add_argument('--airr-output', action='store_true', help='Write output file(s) in AIRR-C format (if --outfname has suffix .tsv, only the airr .tsv is written; however if --outfname has suffix .yaml, both the standard partis .yaml file and an airr .tsv are written). A description of the airr columns can be found here https://docs.airr-community.org/en/stable/datarep/rearrangements.html#fields.')
parent_parser.add_argument('--airr-input', action='store_true', help='Read --infname in airr format. Equivalent to setting \'--seq-column sequence --name-column sequence_id\'.')
parent_parser.add_argument('--extra-annotation-columns', help='Extra columns to add to the (fairly minimal) set of information written by default to annotation output files (choose from: %s' % ' '.join(utils.extra_annotation_headers))  # NOTE '-columns' in command line arg, but '-headers' in utils (it's more consistent that way, I swear)
parent_parser.add_argument('--cluster-annotation-fname', help='output file for cluster annotations (default is <--outfname>-cluster-annotations.<suffix>)')
parent_parser.add_argument('--parameter-dir', help='Directory to/from which to write/read sample-specific parameters. If not specified, a default location is used (and printed to std out). If it does not exist, we infer parameters before proceeding to the desired action.')
parent_parser.add_argument('--light-chain-parameter-dir', help='directory for light chain parameters when --paired-loci is set')
parent_parser.add_argument('--parameter-type', default='hmm', choices=processargs.parameter_type_choices, help='Use parameters from Smith-Waterman (sw) or the HMM (hmm) subdirectories for inference/simulation? (you should almost certainly use the hmm ones, but sw is occasionally useful for debugging)')
parent_parser.add_argument('--parameter-out-dir', help='Special parameter dir for writing multi-hmm parameters, i.e. when running annotate or partition with --count-parameters set (if not set, defaults to <--parameter-dir>/multi-hmm).')
parent_parser.add_argument('--refuse-to-cache-parameters', action='store_true', help='Disables auto parameter caching, i.e. if --parameter-dir doesn\'t exist, instead of inferring parameters, raise an exception. Useful for batch/production use where you want to make sure you\'re caching parameters in a separate step.')
parent_parser.add_argument('--persistent-cachefname', help='Name of file which will be used as an initial cache file (if it exists), and to which all cached info will be written out before exiting.')
parent_parser.add_argument('--sw-cachefname', help='Smith-Waterman cache file name. Default is set using a hash of all the input sequence ids (in partitiondriver, since we have to read the input file first).')
parent_parser.add_argument('--write-sw-cachefile', action='store_true', help='Write sw results to the sw cache file during actions for which we\'d normally only look for an existing one (i.e annotate and partition).')
parent_parser.add_argument('--workdir', help='Temporary working directory (default is set below)')

parent_parser.add_argument('--plotdir', help='Base directory to which to write plots (by default this is not set, and consequently no plots are written')
parent_parser.add_argument('--plot-annotation-performance', action='store_true', help='Compare inferred and true annotation values (deletion lengths, gene calls, etc.). If --plotdir is set, write corresponding plots to disk, and see also --print-n-worst-annotations.')
parent_parser.add_argument('--print-n-worst-annotations', type=int, help='For use with --plot-annotation-performance: print ascii annotations for the N least accurate annotations according to several annotation values (e.g. VD insertion length, distance to true naive sequence). One of two ways to visualize annotation performance -- the other is by setting --plotdir to look at summary plots over all families. View with "less -RS".')
parent_parser.add_argument('--no-partition-plots', action='store_true', help='don\'t make paritition plots, even if --plotdir is set (presumably because you want other plots, e.g. for selection metrics -- the partition plots have a lot more depenencies)')
parent_parser.add_argument('--only-csv-plots', action='store_true', help='skip writing actual image files, which can quite be slow, and only write the csv/yaml summaries (where implemented)')
parent_parser.add_argument('--make-per-gene-plots', action='store_true', help='in addition to plots aggregating over genes, write plots displaying info for each gene of, e.g., per position shm rate, deletion frequencies')
parent_parser.add_argument('--make-per-gene-per-base-plots', action='store_true', help='in addition to the plots made by --make-per-gene-plots, also make the per-gene, per-base plots (i.e. showing A->T vs A->G (this is quite slow, like a few seconds per gene plot).')
parent_parser.add_argument('--ete-path', default=('/home/%s/anaconda_ete/bin' % os.getenv('USER')) if os.getenv('USER') is not None else None)

parent_parser.add_argument('--default-initial-germline-dir', default=partis_dir + '/data/germlines', help='For internal use only. To specify your own germline directory from which to start, use --initial-germline-dir instead.')
parent_parser.add_argument('--initial-germline-dir', help='Directory with fastas from which to read germline set. Only used when caching parameters, during which its contents is copied into --parameter-dir, perhaps (i.e. if specified) with modification. NOTE default is set below, because control flow is too complicated for argparse')
parent_parser.add_argument('--sanitize-input-germlines', action='store_true', help='By default we require all gene names to look like IGHVx*y. If you set this option when you\'re also setting --initial-germline-dir, then we instead allow arbitrary strings as gene names in this input germline directory, and we sanitize them by adding the correct locus and region info at the start (so make sure that you have --locus set correctly), and adding an arbitrary allele string at the end (e.g. <stuff> --> IGHV<stuff>*x).')
parent_parser.add_argument('--simulation-germline-dir', help='Germline directory that was used for simulation (used if --is-simu is set, altough if the default germline info is similar to that used for simulation it may not be necessary)')
parent_parser.add_argument('--aligned-germline-fname', help='fasta file with imgt-gapped alignments for each V, D, and J gene (used by --presto-output). The defaults are in data/germlines/<species>/imgt-alignments/ (set in processargs.py). The existing alignments are automatically extended with any missing genes, but no guarantees are made as to the similarity of this to what imgt would do.')
parent_parser.add_argument('--dtr-path', help='path to directory with decision tree regression model files')  # , default=partis_dir + '/data/selection-metrics/dtr-models'

parent_parser.add_argument('--n-max-alleles-per-gene', type=int, default=None, help='if set, after allele inference the germline set is reduced such that each imgt-name-defined gene has at most this many alleles, with the alleles assigned to the fewest sequences removed first.')
parent_parser.add_argument('--typical-genes-per-region-per-subject', default='55:25:6', help='typical number of alleles per subject for the v, d, and j regions (used by --min-allele-prevalence-fraction)')
parent_parser.add_argument('--min-allele-prevalence-fraction', type=float, default=0.0005, help='Remove any V alleles that represent less than this fraction of the repertoire (rescaled using --typical-genes-per-region-per-subject for D and J). Converted to --min-allele-prevalence-fractions (note plural!) internally.')

parent_parser.add_argument('--leave-default-germline', action='store_true')
parent_parser.add_argument('--dont-remove-unlikely-alleles', action='store_true')
parent_parser.add_argument('--allele-cluster', action='store_true')
parent_parser.add_argument('--kmeans-allele-cluster', action='store_true')
parent_parser.add_argument('--dont-find-new-alleles', action='store_true')
# parent_parser.add_argument('--always-find-new-alleles', action='store_true', help='By default we only look for new alleles if a repertoire\'s mutation rate is amenable to reasonable new-allele sensitivity (i.e. if it\'s not crazy high). This overrides that.')
parent_parser.add_argument('--debug-allele-finding', action='store_true', help='print lots of debug info on new-allele fits')
parent_parser.add_argument('--new-allele-fname', help='fasta fname to which to write any new alleles (they are also written, together with previously-known alleles that are also in the sample, to --parameter-dir)')
parent_parser.add_argument('--n-max-snps', type=int, default=8, help='when new-allele finding, look for new V alleles separated from existing V alleles by up to this many SNPs. Also used for allele removal (corresponding numbers for D and J are set automatically)')
parent_parser.add_argument('--n-max-mutations-per-segment', type=int, default=23, help='when new-allele finding, exclude sequences which have more than this many mutations in the V segment')
parent_parser.add_argument('--min-allele-finding-gene-length', type=int, default=150, help='if (after excluding particularly short reads) the reads for a gene are shorter than this, then don\'t look for new alleles with/on this gene')
parent_parser.add_argument('--plot-and-fit-absolutely-everything', type=int, help='fit every single position for this <istart> and write every single corresponding plot (slow as hell, and only for debugging/making plots for paper)')
parent_parser.add_argument('--cluster-indices', help='indices of clusters (when sorted largest to smallest) to print for the \'view-output\' action. Specified as a colon-separated list, where each item can be either a single integer or a python slice-style range of integers, e.g. 0:3-6:50 --> 0:3:4:5:50')
parent_parser.add_argument('--min-selection-metric-cluster-size', type=int, default=treeutils.default_min_selection_metric_cluster_size, help='don\'t calculate selection metrics for clusters smaller than this')
parent_parser.add_argument('--treefname', help='newick-formatted file with a tree corresponding to the sequences either in --infname (if making new output, i.e. action is annotate or partition) or --outfname (if reading existing output, i.e. action is get-selection-metrics).')
parent_parser.add_argument('--selection-metric-fname', help='yaml file to which to write selection metrics. If not set, and if --add-metrics-to-outfname is not set, this defaults to <--outfname>.replace(<suffix>, \'-selection-metrics\' + <suffix>)')
parent_parser.add_argument('--add-selection-metrics-to-outfname', action='store_true', help='If set, instead of writing a separate file with selection metrics, we include them in <--outfname> under the key \'tree-info\'.')
parent_parser.add_argument('--lb-tau', type=float, default=treeutils.default_lb_tau, help='exponential decay length for local branching index (lbi). See also --lbr-tau-factor.')  # default is set in treeutils so we can import it in bin/get-tree-metrics.py UPDATE deleted that script, so could probably move this
parent_parser.add_argument('--lbr-tau-factor', type=int, default=treeutils.default_lbr_tau_factor, help='factor by which to multiply --lb-tau in order to get the exponential decay length (tau) for local branching ratio (lbr). The ratio (lbr) should, in general, have tau much larger than for the index (lbi). See also --lb-tau.')  # default is set in treeutils so we can import it in bin/get-selection-metrics.py UPDATE deleted that script, so could probably move this
parent_parser.add_argument('--dont-normalize-lbi', action='store_true', help='By default, we normalize tau so that 0. is a rough minimum, and 1. is a fairly large value. But since you can\'t normalize for tau larger than 1/seq_len, it\'s useful to turn off normalization.')
parent_parser.add_argument('--include-relative-affy-plots', action='store_true', help='in addition to validation plots using actual affinity from simulation, also make plots using \'relative\' affinity (see bcr-phylo), i.e. a cell\'s affinity relative only to other cells alive at the same point in time. Useful because the selection metrics attempt to predict the effects of fitness on branchiness only at a given time (or: if you sample a lot of intermediate ancestors, your selection metric performance will look artificially bad because the ancestors, while having higher affinity than other cells alive at the same time, have affinity lower than the leaves).')

parent_parser.add_argument('--no-sw-vsearch', action='store_true', help='By default, we get preliminary V annotations from vsearch that we pass to sw. This improves accuracy because the vsearch shm rate estimates let us separate sequences into groups by optimal sw match/mismatch scores, and improves speed by letting us reverse shm V indels before running sw (so fewer sw iterations; also vsearch is faster than sw). Setting this option skips the preliminary vsearch step (although if caching parameters, we still use vsearch to remove less-likely genes; but no vsearch info is passed to sw)')

# ----------------------------------------------------------------------------------------
subconfig = collections.OrderedDict((
    ('version'          , {'func' : int, 'help' : 'print version information and exit'}),  # int doesn't do anything, it's just because I have to put something here
    ('cache-parameters' , {'func' : run_partitiondriver, 'help' : 'Cache parameter values and write hmm model files.'}),
    ('annotate'         , {'func' : run_partitiondriver, 'help' : 'Annotate sequences in input file, i.e. run the viterbi algorithm, using pre-existing parameter directory.'}),
    ('partition'        , {'func' : run_partitiondriver, 'help' : 'Partition sequences in input file into clonally-related families using pre-existing parameter directory.'}),
    ('simulate'         , {'func' : run_simulation,      'help' : 'Generate simulated sequences based on information in pre-existing parameter directory.'}),
    ('view-output'      , {'func' : run_partitiondriver, 'help' : 'Print partitions and/or annotations from an existing output yaml file.'}),
    ('merge-paired-partitions', {'func' : run_partitiondriver, 'help' : 'Merge the heavy and light chain partition paths from two existing output files.'}),
    ('view-alternative-annotations' , {'func' : run_partitiondriver, 'help' : 'Print (to std out) a comparison of the naive sequences and v/d/j gene calls corresponding to sub- and super-clusters of the cluster specified with --queries. You must have specified --calculate-alternative-naive-seqs in a previous partition step so that this information was saved.'}),
    ('plot-partitions'  , {'func' : run_partitiondriver, 'help' : 'Plot existing partitions and cluster annotations.'}),
    ('get-selection-metrics' , {'func' : run_partitiondriver, 'help' : 'Calculate selection metrics using existing output.'}),
    ('get-linearham-info', {'func' : run_partitiondriver, 'help' : 'Write input file for linearham (to --linearham-info-fname), using a previous partis output (--outfname) file as input.'}),
    # deprecated actions:
    ('view-annotations' , {'func' : run_partitiondriver, 'help' : 'Mostly deprecated: Print annotations from an existing old-style annotation output csv (for current yaml output files, use \'view-output\').'}),
    ('view-partitions'  , {'func' : run_partitiondriver, 'help' : 'Mostly deprecated: Print partitions from an existing old-style partition output csv (for current yaml output files, use \'view-output\').'}),
    ('view-alternative-naive-seqs'  , {'func' : run_partitiondriver, 'help' : 'DEPRECATED use \'view-alternative-annotations\''}),
    ('run-viterbi'      , {'func' : run_partitiondriver, 'help' : 'DEPRECATED use \'annotate\''}),
    ('get-tree-metrics' , {'func' : run_partitiondriver, 'help' : 'DEPRECATED use get-selection-metrics'}),
))

def runs_on_existing_output(actionstr):
    return actionstr.split('-')[0] in ['view', 'plot', 'get', 'merge']

subargs = {subname : [] for subname in subconfig}

# ----------------------------------------------------------------------------------------
subargs['annotate'].append({'name' : '--get-selection-metrics', 'kwargs' : {'action' : 'store_true', 'help' : 'calculate tree-based selection metrics for each cluster.'}})

# ----------------------------------------------------------------------------------------
subargs['partition'].append({'name' : '--naive-hamming', 'kwargs' : {'action' : 'store_true', 'help' : 'agglomerate purely with naive hamming distance, i.e. set the low and high preclustering bounds to the same value'}})
subargs['partition'].append({'name' : '--naive-vsearch', 'kwargs' : {'action' : 'store_true', 'help' : 'Very fast clustering: infer naive (unmutated ancestor) for each input sequence, then toss it all into vsearch. But, of course, not as accurate as the slower methods.'}})
subargs['partition'].append({'name' : '--seed-unique-id', 'kwargs' : {'help' : 'Throw out all sequences that are not clonally related to this sequence id. Much much much faster than partitioning the entire sample (well, unless your whole sample is one family).'}})  # NOTE do *not* move these up above -- we forbid people to set them for auto parameter caching (see exception above)
subargs['partition'].append({'name' : '--seed-seq', 'kwargs' : {'help' : 'same effect as --seed-unique-id, but specifies the sequence instead of that sequence\'s id (so that it doesn\'t have to be in the original input file)'}})
subargs['partition'].append({'name' : '--random-seed-seq', 'kwargs' : {'action' : 'store_true', 'help' : 'choose a sequence at random from the input file, and use it as the seed for seed partitioning (as if it had been set as the --seed-unique-id)'}})
subargs['partition'].append({'name' : '--annotation-clustering', 'kwargs' : {'help' : 'Perform annotation-based clustering: group together sequences with the same V and J, same CDR3 length, and 90%% cdr identity. Very, very inaccurate.'}})
subargs['partition'].append({'name' : '--annotation-clustering-thresholds', 'kwargs' : {'default' : '0.9', 'help' : 'colon-separated list of thresholds for annotation-based clustering (i.e. a cluster is defined as the same v and j gene plus this threshold on cdr3 nucleotide hamming distance)'}})
subargs['partition'].append({'name' : '--naive-hamming-bounds', 'kwargs' : {'help' : 'Clustering bounds (lo:hi colon-separated pair) on naive sequence hamming distance. If not specified, the bounds are set based on the per-dataset mutation levels. For most purposes should be left at the defaults.'}})
subargs['partition'].append({'name' : '--logprob-ratio-threshold', 'kwargs' : {'type' : float, 'default' : 18., 'help' : 'reaches a min value of <this> minus five for large clusters.'}})
subargs['partition'].append({'name' : '--synthetic-distance-based-partition', 'kwargs' : {'action' : 'store_true', 'help' : 'Use simulation truth info to create a synthetic distance-based partition (for validation).'}})
subargs['partition'].append({'name' : '--cache-naive-hfracs', 'kwargs' : {'action' : 'store_true', 'help' : 'In addition to naive sequences and log probabilities, also cache naive hamming fractions between cluster pairs. Only really useful for plotting or testing.'}})
subargs['partition'].append({'name' : '--n-precache-procs', 'kwargs' : {'type' : int, 'help' : 'Number of processes to use when precaching naive sequences. Default is set based on some heuristics, and should typically only be overridden for testing.'}})
subargs['partition'].append({'name' : '--biggest-naive-seq-cluster-to-calculate', 'kwargs' : {'type' : int, 'default' : 15, 'help' : 'start thinking about subsampling before you calculate anything if cluster is bigger than this'}})
subargs['partition'].append({'name' : '--biggest-logprob-cluster-to-calculate', 'kwargs' : {'type' : int, 'default' : 15, 'help' : 'start thinking about subsampling before you calculate anything if cluster is bigger than this'}})
subargs['partition'].append({'name' : '--n-partitions-to-write', 'kwargs' : {'type' : int, 'default' : 10, 'help' : 'Number of partitions (surrounding the best partition) to write to output file.'}})
subargs['partition'].append({'name' : '--naive-swarm', 'kwargs' : {'action' : 'store_true', 'help' : 'Use swarm instead of vsearch, which the developer recommends. Didn\'t seem to help much, and needs more work to optimize threshold, so DO NOT USE.'}})
subargs['partition'].append({'name' : '--small-clusters-to-ignore', 'kwargs' : {'help' : 'colon-separated list (or dash-separated inclusive range) of cluster sizes to throw out after several partition steps. E.g. \'1:2\' will, after <--n-steps-at-which-to-ignore-small-clusters> partition steps, throw out all singletons and pairs. Alternatively, \'1-10\' will ignore all clusters with size less than 11.'}})
subargs['partition'].append({'name' : '--n-steps-after-which-to-ignore-small-clusters', 'kwargs' : {'type' : int, 'default' : 3, 'help' : 'number of partition steps after which to throw out small clusters (where "small" is controlled by <--small-clusters-to-ignore>). (They\'re thrown out before this if we get to n_procs one before this).'}})
subargs['partition'].append({'name' : '--n-final-clusters', 'kwargs' : {'type' : int, 'help' : 'If you reach the maximum likelihood partition and there are still more than this many clusters, attempt to keep merging until there aren\'t. NOTE should\'ve called this --n-max-final-clusters, but too late to change now'}})
subargs['partition'].append({'name' : '--min-largest-cluster-size', 'kwargs' : {'type' : int, 'help' : 'If you reach the maximum likelihood partition and the largest cluster isn\'t this big, attempt to keep merging until it is.'}})
subargs['partition'].append({'name' : '--calculate-alternative-annotations', 'kwargs' : {'action' : 'store_true', 'help' : 'write to disk all the information necessary to, in a later step (\'view-alternative-annotations\'), print alternative inferred naive sequences (i.e. visualize uncertainty in the inferred naive sequence). This is largely equivalent to setting --write-additional-cluster-annotations to \'sys.max_int:sys.max_int\'.'}})
subargs['partition'].append({'name' : '--calculate-alternative-naive-seqs', 'kwargs' : {'action' : 'store_true', 'help' : 'DEPRECATED use --calculate-alternative-annotations'}})
subargs['partition'].append({'name' : '--max-cluster-size', 'kwargs' : {'type' : int, 'help' : 'stop clustering immediately if any cluster grows larger than this (useful for limiting memory usage, which can become a problem when the final partition contains very large clusters)'}})
subargs['partition'].append({'name' : '--write-additional-cluster-annotations', 'kwargs' : {'help' : 'in addition to writing annotations for each cluster in the best partition, also write annotations for all the clusters in several partitions on either side of the best partition. Specified as a pair of numbers \'m:n\' for m partitions before, and n partitions after, the best partition.'}})
subargs['partition'].append({'name' : '--get-selection-metrics', 'kwargs' : {'action' : 'store_true', 'help' : 'calculate tree-based selection metrics for each cluster (can also be run on existing output with \'get-selection-metrics\' action). By default it calculates fast, but very approximate, trees (see manual for details), but you can give it more accurate trees by setting --treefname (unrelated to --input-simulation-treefname).'}})
subargs['partition'].append({'name' : '--get-tree-metrics', 'kwargs' : {'action' : 'store_true', 'help' : 'DEPRECATED use --get-selection-metrics'}})

# ----------------------------------------------------------------------------------------
# basic simulation:
subargs['simulate'].append({'name' : '--mutation-multiplier', 'kwargs' : {'type' : float, 'help' : 'Multiply observed branch lengths by some factor when simulating, e.g. if in data it was 0.05, but you want closer to ten percent in your simulation, set this to 2'}})
subargs['simulate'].append({'name' : '--n-sim-events', 'kwargs' : {'type' : int, 'default' : 1, 'help' : 'Number of rearrangement events to simulate'}})
subargs['simulate'].append({'name' : '--n-trees', 'kwargs' : {'type' : int, 'help' : 'Number of phylogenetic trees from which to choose during simulation (we pre-generate this many trees before starting a simulation run, then for each rearrangement event choose one at random -- so this should be at least of order the number of simulated events, so your clonal families don\'t all have the same tree).'}})
subargs['simulate'].append({'name' : '--n-leaves', 'kwargs' : {'type' : float, 'default' : 5., 'help' : 'Parameter determining the distribution from which the number of leaves per tree is drawn (--n-leaf-distribution). For \'geometric\' and \'box\', this is the mean; while for \'zipf\' it is the exponent, i.e. *not* the mean -- see docs for numpy.random.zipf for details.'}})
subargs['simulate'].append({'name' : '--constant-number-of-leaves', 'kwargs' : {'action' : 'store_true', 'help' : 'Give all trees the same number of leaves'}})
subargs['simulate'].append({'name' : '--allowed-cdr3-lengths', 'kwargs' : {'help' : 'Colon-separated list of cdr3 lengths to which to restrict the simulation. NOTE that our cdr3 definition includes both conserved codons, which differs from the imgt definition (sorry).'}})
subargs['simulate'].append({'name' : '--remove-nonfunctional-seqs', 'kwargs' : {'action' : 'store_true', 'help' : 'Remove non-functional sequences from simulated rearrangement events. Note that because this happens after generating SHM (since we have no way to tell bppseqgen to only generate functional sequences), you will in general need to specify a (potentialy much) larger value for --n-leaves if you set --remove-nonfunctional-seqs. Typically, the vast majority of nonfunctional simulated sequences are due to stop codons generated by SHM.'}})
subargs['simulate'].append({'name' : '--n-leaf-distribution', 'kwargs' : {'default' : 'geometric', 'choices' : ['geometric', 'box', 'zipf'], 'help' : 'distribution from which to draw the number of leaves for each tree (the parameter comes from --n-leaves)'}})
subargs['simulate'].append({'name' : '--gtrfname', 'kwargs' : {'default' : partis_dir + '/data/recombinator/gtr.txt', 'help' : 'File with list of GTR parameters. Fed into bppseqgen along with the chosen tree. Corresponds to an arbitrary dataset at the moment, but eventually will be inferred per-dataset.'}})  # NOTE command to generate gtr parameter file: [stoat] partis/ > zcat /shared/silo_researcher/Matsen_F/MatsenGrp/data/bcr/output_sw/A/04-A-M_gtr_tr-qi-gi.json.gz | jq .independentParameters | grep -v '[{}]' | sed 's/["\:,]//g' | sed 's/^[ ][ ]*//' | sed 's/ /,/' | sort >data/gtr.txt)
subargs['simulate'].append({'name' : '--root-mrca-weibull-parameter', 'kwargs' : {'type' : float, 'help' : 'if set, uses TreeSimGM (instead of TreeSim), and passes this value as the weibull parameter (e.g. 0.1: long root-mrca distance, lots of shared mutation; 5: short, little) NOTE requires installation of TreeSimGM'}})
subargs['simulate'].append({'name' : '--input-simulation-treefname', 'kwargs' : {'help' : 'file with newick-formatted lines corresponding to trees to use for simulation. Note that a) the tree depths are rescaled according to the shm rates requested by other command line arguments, i.e. the depths in the tree file are ignored, and b) the resulting sequences do not use the leaf names from the trees (unrelated to --treefname).'}})
subargs['simulate'].append({'name' : '--generate-trees', 'kwargs' : {'action' : 'store_true', 'help' : 'Run the initial tree-generation step of simulation (writing to --outfname), without then proceeding to actually generate the sequences. Used for paired heavy/light simulation so we can pass the same list of trees to both.'}})
subargs['simulate'].append({'name' : '--choose-trees-in-order', 'kwargs' : {'action' : 'store_true', 'help' : 'Instead of the default of choosing a tree at random from the list of trees (which is either generated at the start of the simulation run, or passed in with --input-simulation-treefname), instead choose trees sequentially. If there\'s more events than trees, it cycles through the list again. Used for paired heavy/light simulation.'}})
subargs['simulate'].append({'name' : '--check-tree-depths', 'kwargs' : {'action' : 'store_true', 'help' : 'check how close the fraction of mutated bases that we get from bppseqgen is to the depth of the trees that we passed in.'}})
subargs['simulate'].append({'name' : '--no-per-base-mutation', 'kwargs' : {'action' : 'store_true', 'help' : 'By default the simulation uses both a different SHM rate for each position in each gene, and also a different rate to each base for each of these positions (e.g. position 37 in IGHV1-2*02 might mutate 3x as much as neighboring bases, and it might mutate to T twice as often as G). This option turns off the per-base part of that, so while each position in each gene still has a different overall rate, the rate to each base is the same (e.g. A->T is the same as A->G). Run time with this option set is about 10 times faster than the default. NOTE also --no-per-base-mfreqs'}})
subargs['simulate'].append({'name' : '--mutate-conserved-codons', 'kwargs' : {'action' : 'store_true', 'help' : 'By default we don\'t let mutations occur in conserved codons (cyst in V and tryp/phen in J), but if this option is set we let them mutate.'}})
# shm indels:
subargs['simulate'].append({'name' : '--indel-frequency', 'kwargs' : {'default' : 0., 'type' : float, 'help' : 'fraction of simulated sequences with SHM indels'}})
subargs['simulate'].append({'name' : '--n-indels-per-indeld-seq', 'kwargs' : {'default' : '1:2', 'help' : 'list of integers from which to choose the number of SHM indels in each sequence that we\'ve already decided has indels'}})
subargs['simulate'].append({'name' : '--mean-indel-length', 'kwargs' : {'type' : float, 'default' : 5, 'help' : 'mean length of each SHM indel (geometric distribution)'}})
subargs['simulate'].append({'name' : '--indel-location', 'kwargs' : {'help' : 'Where to put simulated SHM indels. Set either to a single integer position at which all indels will occur, or choose from one of three regions (each excluding the first and last five positions in the sequence): None (anywhere in sequence), \'v\' (before cysteine), \'cdr3\' (within cdr3). If set to a single position, any entries in --n-indels-per-indeld-seq greater than 1 are removed, since we don\'t want a bunch of them at the same point.'}})
# from-scratch (rather than mimicking a particular data sample):
subargs['simulate'].append({'name' : '--rearrange-from-scratch', 'kwargs' : {'action' : 'store_true', 'help' : 'Don\'t use an existing parameter directory for rearrangement-level parameters, and instead make up some plausible stuff from scratch. Have to also set --shm-parameter-dir.'}})
subargs['simulate'].append({'name' : '--mutate-from-scratch', 'kwargs' : {'action' : 'store_true', 'help' : 'Don\'t use an existing parameter directory for shm-level (mutation) parameters, and instead make up stuff from scratch (by default this means shm rate varies over positions and sequences, but is the same for all regions). Have to also set --reco-parameter-dir.'}})
subargs['simulate'].append({'name' : '--simulate-from-scratch', 'kwargs' : {'action' : 'store_true', 'help' : 'same as setting both --rearrange-from-scratch and --mutate-from-scratch'}})
subargs['simulate'].append({'name' : '--shm-parameter-dir', 'kwargs' : {'help' : 'parameter directory from which to retrieve shm-level info when --rearrange-from-scratch is set (to set germline info, use --initial-germline-dir).'}})
subargs['simulate'].append({'name' : '--reco-parameter-dir', 'kwargs' : {'help' : 'parameter directory from which to retrieve rearrangement-level info when --mutate-from-scratch is set (to set germline info, use --initial-germline-dir).'}})
subargs['simulate'].append({'name' : '--scratch-mute-freq', 'kwargs' : {'type' : float, 'default' : 0.05, 'help' : 'shm rate used by --mutate-from-scratch'}})
subargs['simulate'].append({'name' : '--flat-mute-freq', 'kwargs' : {'action' : 'store_true', 'help' : 'use the same shm rate (--scratch-mute-freq) for all positions (in practice it\'s not that much flatter than the Gamma that is used by default --mutate-from-scratch). For use with --mutate-from-scratch.'}})
subargs['simulate'].append({'name' : '--same-mute-freq-for-all-seqs', 'kwargs' : {'action' : 'store_true', 'help' : 'use the same shm rate (--scratch-mute-freq) for all sequences. For use with --mutate-from-scratch. NOTE: this means we tell bppseqgen to use the same expected rate for every sequence -- there\'s still variance in the resulting number of output mutation per sequence.'}})
# new allele/germline set generation (this is also for from-scratch)
subargs['simulate'].append({'name' : '--generate-germline-set', 'kwargs' : {'action' : 'store_true', 'help' : 'Choose a realistic germline set from the available genes. Turned on automatically when --rearrange-from-scratch or --simulate-from-scratch are set.'}})
subargs['simulate'].append({'name' : '--force-dont-generate-germline-set', 'kwargs' : {'action' : 'store_true', 'help' : 'Force --generate-germline-set to False in situations in which it would otherwise be turned on (I know, this is kind of messy, but I think it\'s the best available solution).'}})
subargs['simulate'].append({'name' : '--n-genes-per-region', 'kwargs' : {'help' : 'colon-separated list specifying the number of genes (not alleles -- i.e. the *total* number of alleles is this times the number of alleles per gene) for each region (for use with --generate-germline-set). Default (set outside argparse, so argparse incorrectly thinks it\'s None): %s' % glutils.default_n_genes_per_region}})
subargs['simulate'].append({'name' : '--n-sim-alleles-per-gene', 'kwargs' : {'help' : 'colon-separated list of mean alleles per gene for each region (for use with --generate-germline-set). Default (set outside argparse, so argparse incorrectly thinks it\'s None): %s' % glutils.default_n_alleles_per_gene}})
subargs['simulate'].append({'name' : '--min-sim-allele-prevalence-freq', 'kwargs' : {'default' : glutils.default_min_allele_prevalence_freq,'type' : float, 'help' : 'minimum frequency at which alleles are allowed to occur, e.g. if it\'s 0.01 then each pair of V alleles will have a prevalence ratio between 0.01 and 1'}})
subargs['simulate'].append({'name' : '--allele-prevalence-fname', 'kwargs' : {'help' : 'abandon help all ye who enter here'}})
subargs['simulate'].append({'name' : '--nsnp-list', 'kwargs' : {'help' : 'When --generate-germline-set is set, this is a colon-separated list whose length gives the number of novel alleles to add to the germline set. Each entry in the list is the number of SNPs to generate for the corresponding new allele. If both --nsnp-list and --nindel-list are set, they must be the same length; if one is unset, it is assumed to be all zeroes.'}})
subargs['simulate'].append({'name' : '--nindel-list', 'kwargs' : {'help' : 'When --generate-germline-set is set, this is a colon-separated list whose length gives the number of novel alleles to add to the germline set. Each entry in the list is the number of indels to generate for the corresponding new allele. If both --nsnp-list and --nindel-list are set, they must be the same length; if one is unset, it is assumed to be all zeroes.'}})
subargs['simulate'].append({'name' : '--im-a-subproc', 'kwargs' : {'action' : 'store_true', 'help' : 'Internal use only. This is set to true if this is a subprocess, i.e. set when --n-procs > 1.'}})

subargs['view-alternative-annotations'].append({'name' : '--print-all-annotations', 'kwargs' : {'action' : 'store_true', 'help' : 'In addition to printing each alternative naive seq and gene call, with information on which clusters support its correctness, also print the annotations for all of these clusters below each naive sequence and gene call.'}})
subargs['view-alternative-annotations'].append({'name' : '--seed-unique-id', 'kwargs' : {'help' : 'see help for this option under \'partition\' action'}})  # NOTE do *not* move these up above -- we forbid people to set them for auto parameter caching (see exception above)
subargs['view-output'].append({'name' : '--seed-unique-id', 'kwargs' : {'help' : 'see help for this option under \'partition\' action'}})  # NOTE do *not* move these up above -- we forbid people to set them for auto parameter caching (see exception above)
subargs['view-output'].append({'name' : '--extra-print-key', 'kwargs' : {'help' : 'add the value of this key to the ascii art annotation'}})
subargs['get-selection-metrics'].append({'name' : '--seed-unique-id', 'kwargs' : {'help' : 'see help for this option under \'partition\' action'}})  # NOTE do *not* move these up above -- we forbid people to set them for auto parameter caching (see exception above)
subargs['get-tree-metrics'].append({'name' : '--seed-unique-id', 'kwargs' : {'help' : 'DEPRECATED use the \'get-selection-metrics\' action'}})  # NOTE do *not* move these up above -- we forbid people to set them for auto parameter caching (see exception above)

subargs['get-linearham-info'].append({'name' : '--linearham-info-fname', 'kwargs' : {'help' : 'yaml file to which to write linearhmam input information'}})

def get_arg_names(actions):  # return set of all arg names (in the form they appear in args.__dict__) for the specified actions
    if actions == 'all':
        actions = subconfig.keys()
    return set([actionconf['name'][2:].replace('-', '_') for action in actions for actionconf in subargs[action]])

# above we just made a dict with lists of args, here we actually make the sub parsers
subparsermap = {}
for name, vals in subconfig.items():
    subparsermap[name] = subparsers.add_parser(name, parents=[parent_parser], help=vals['help'], formatter_class=MultiplyInheritedFormatter)
    subparsermap[name].set_defaults(func=vals['func'])  # set the default fcn to run for <name> (i.e. action)
    for argconf in subargs[name]:
        subparsermap[name].add_argument(argconf['name'], **argconf['kwargs'])

# ----------------------------------------------------------------------------------------
args = parser.parse_args()

# add OR of all arguments to all subparsers to <args>, as None (to avoid having to rewrite a *##!(%ton of other code)
for missing_arg in get_arg_names('all') - set(args.__dict__):  # NOTE see also remove_action_specific_args() above, which kind of does similar things, at least if i rewrote this all from scratch only one (or neither...) would exist
    args.__dict__[missing_arg] = None

if args.outfname is None and args.paired_loci_output_dir is None and runs_on_existing_output(args.action):  # would be better to have this in processargs.py like everything else, but then I'd have to import the function somehow
    raise Exception('--outfname required for %s' % args.action)

processargs.process(args)
random.seed(args.seed)
numpy.random.seed(args.seed)
start = time.time()
args.func(args)
print '      total time: %.1f' % (time.time()-start)
